

- reorganize rule results to be more meaningfull
   right now they are exactly what the upload returns
   they can be better organized
- test the case that the control is also the target
- the pip module python-magic doesn't have and 'open' attribute that the action plugin needs
    change the code to just always use plan b.
    Ansible itself seems to use a variant of plan b.
    rename plan b to just get_mime_type.
- RPM installation of skelleton that installs scripts that act like current
  client, but actually use magpie
- download specs file from server, look for specs file on target machine and
  controler machine,
- add option for specifying specs?
- add option to action plugin to specify a file to keep/store the archive,
    delete the archive unless this option is used
- since modules can't do inports, need to have a build system
   that builds the actual modules from seperate files
    - read the current uploader.json and produce the action and library module
    - also have a mechinism for includeing arbitrary but idiomatic python to
        collect arbitrary facts.
- add testing to compare archive created to archive created by original client
  - add comparison to last run to test.sh
  - add comparison of 'skipped' to test.sh
- add testing to compare rule hits produced by old client and this client
- fix documentation for both modules
- fact name should be based on spec name, not archive file name
  the fact names used to return data from the target to the control machine
    are variations of 'magpie_' + <archive file name where data is to go>
    this should be changed to 'magpie_' + spec_name so that they are
    easier to read
    - in order to do this for old-style specs, we have to map old style specs
      into new style specs,
      - if the old style spec doesn't have a coorisponding new style spec,
        which probabably never happens because of the way we create the specs
        file, though may happen for specs read from disk,
        just use the last name in the archive file name
- fix _make_sed_cmd to check if default_sed_file exists, use it if it does,
     but use constant command line if it does not
- split into two differently named modules
  - come up with names for both modules (insights and magpie)
  - one collects but doesn't send to red hat
     this one only outputs collected facts
  - one runs the collector and sends to red hat
     this one outputs collected facts, but also the rule results
     returned as facts
- default_ca_file (and anything that uses it)
  This file contains a cert that the client uses to verify that the server is valid.
  We currently set it to false which means that the client doesn't verify the server.
  Instead we should check to see if the cert exists on the control machine, and
  verify the server if it exists.
- figure out what to do about rhsm cert
  - what do other bits of software do, what does the RHSM documentation say to do
     - is the rhsm cert only for use on registered machines or can one get an rhsm cert
       for an organization that can be installed anywhere in an organization and used
       for commuinicating with the portal?
  - the current client calls into some rhsm supplied code to get the names of
    two files that it passes on to the http request code
- handle 'pre-commands'
- move do_upload failure messages out of log and into Ansible failure results
- move do_upload results out of 'log' and into returned facts
- rewrite 'test_connection' to return results in stderr (or stderr)
- figure out what to do about logging during fact collection
  - the current collector does a lot of logging during collection, while collection
    commands are running, and then ships that log back with the archive.
    So during fact collection, log stuff to ansible normally, and then capture those
    logs in the action_plugin, and put them in the archive.
- take config information from module options
   - correct the ansible argument parser in library
- add registration to magpie
    for the time being we must store the system id on the system itself
    the action plugin needs to contact the system to see if it is registered
      if there are easy ways to grab files off of target systems then use
      that, otherwise
        when the module is called on an unregistered system, it returns
        an error, and then the action plugin notices this error, registers
        the system, and recalls the module with the system id as a parameter
        the module stores the incomming system id, and then collects and
        returns
    do the call to the register api in the action plugin
    then write the configure files to the target machine
- create a system id mechinism that doesn't require storeing uuid's on
  systems
- handle sat5 branch_info
- specs black/white lists
   - honor existing black/white lists on individual hosts
   - have a mechinism for black/white list in ansible (on control machine)
